# Policies

In deep reinforcement learning, a policy defines the way an agent behaves by determining how it selects actions based on the state of the environment. Essentially, the policy serves as the agent’s decision-making strategy, guiding it toward achieving its objective over time. Depending on the nature of the problem, a policy can be either deterministic or stochastic. A deterministic policy provides a direct mapping from states to actions, meaning that given the same state, the agent will always take the same action. In contrast, a stochastic policy represents a probability distribution over possible actions, allowing the agent to choose actions based on certain probabilities rather than following a fixed rule. 

Mathematically, a stochastic policy is often expressed as a probability function that assigns a likelihood to each possible action given a particular state. This function, denoted as $\pi(a \mid s)$, represents the probability of selecting action $ a $ when the agent is in state $ s $. In deep reinforcement learning, policies are typically parameterized using neural networks, allowing the agent to learn complex decision-making patterns from experience. By optimizing the parameters of this neural network, the agent refines its policy to maximize cumulative rewards over time. 

$$\pi: S \rightarrow A$$

Different reinforcement learning approaches handle policy learning in distinct ways. Some methods, such as value-based approaches, do not explicitly maintain a policy but instead derive actions from a learned value function, which estimates the expected reward of each action. Others, particularly policy-based methods, directly optimize the policy function itself, updating its parameters to increase the probability of actions that lead to higher rewards. Actor-critic methods combine both strategies by maintaining a policy function, often referred to as the actor, alongside a value function, known as the critic, which helps guide the learning process by estimating the long-term return of each state-action pair. 

Through iterative interaction with the environment, the policy evolves, improving the agent’s ability to make decisions that yield better outcomes. This continuous refinement, driven by optimization algorithms such as policy gradient methods or trust-region approaches, enables reinforcement learning agents to tackle complex decision-making problems in dynamic and uncertain environments.