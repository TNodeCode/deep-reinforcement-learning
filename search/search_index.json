{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Installation/","title":"Installation of the Deep Reinforcement Learning Library","text":"<ol> <li>Install PyTorch (see https://pytorch.org)</li> <li>Install Swig (see: https://open-box.readthedocs.io/en/latest/installation/install_swig.html)<ul> <li>Windows: <code>choco install swig</code></li> <li>Ubunu: <code>apt-get install swig4.0</code></li> <li>MacOS: <code>brew install swig</code></li> </ul> </li> <li>Install Python dependencies     <pre><code>$ pip install -r requirements.txt\n</code></pre></li> </ol>"},{"location":"basics/01_Policies/","title":"Policies","text":"<p>In deep reinforcement learning, a policy defines the way an agent behaves by determining how it selects actions based on the state of the environment. Essentially, the policy serves as the agent\u2019s decision-making strategy, guiding it toward achieving its objective over time. Depending on the nature of the problem, a policy can be either deterministic or stochastic. A deterministic policy provides a direct mapping from states to actions, meaning that given the same state, the agent will always take the same action. In contrast, a stochastic policy represents a probability distribution over possible actions, allowing the agent to choose actions based on certain probabilities rather than following a fixed rule. </p> <p>Mathematically, a stochastic policy is often expressed as a probability function that assigns a likelihood to each possible action given a particular state. This function, denoted as \\(\\pi(a \\mid s)\\), represents the probability of selecting action $ a $ when the agent is in state $ s $. In deep reinforcement learning, policies are typically parameterized using neural networks, allowing the agent to learn complex decision-making patterns from experience. By optimizing the parameters of this neural network, the agent refines its policy to maximize cumulative rewards over time. </p> \\[\\pi: S \\rightarrow A\\] <p>Different reinforcement learning approaches handle policy learning in distinct ways. Some methods, such as value-based approaches, do not explicitly maintain a policy but instead derive actions from a learned value function, which estimates the expected reward of each action. Others, particularly policy-based methods, directly optimize the policy function itself, updating its parameters to increase the probability of actions that lead to higher rewards. Actor-critic methods combine both strategies by maintaining a policy function, often referred to as the actor, alongside a value function, known as the critic, which helps guide the learning process by estimating the long-term return of each state-action pair. </p> <p>Through iterative interaction with the environment, the policy evolves, improving the agent\u2019s ability to make decisions that yield better outcomes. This continuous refinement, driven by optimization algorithms such as policy gradient methods or trust-region approaches, enables reinforcement learning agents to tackle complex decision-making problems in dynamic and uncertain environments.</p>"},{"location":"basics/02_State-Value%20Functions/","title":"State-Value Functions","text":"<p>In deep reinforcement learning, the state-value function plays a crucial role in evaluating how good it is for an agent to be in a particular state. This function, often denoted as \\(V(s)\\), represents the expected cumulative reward an agent can obtain when starting from state \\(s\\) and following a given policy thereafter. It essentially provides a measure of the long-term desirability of being in a specific state, guiding the agent toward decisions that maximize future rewards. </p> <p>Mathematically, the state-value function under a policy \\(\\pi\\) is defined as the expected sum of discounted rewards when the agent starts in state \\(s\\) and follows the policy \\(\\pi\\) thereafter. This is expressed as:</p> \\[ V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid S_0 = s \\right] \\] <p>where \\(R_t\\) represents the reward received at time step \\(t\\), and \\(\\gamma\\) is the discount factor that determines how much future rewards contribute to the present value. The discount factor, typically set between 0 and 1, ensures that rewards received in the distant future have less impact than immediate rewards, promoting short-term gains while still considering long-term benefits.</p> <p>The state-value function is closely related to the action-value function, which instead evaluates the expected return of taking a specific action in a given state. While the state-value function aggregates over all possible actions dictated by the policy, the action-value function provides a finer-grained evaluation by considering individual actions. Both functions are essential in reinforcement learning, as they help the agent assess the consequences of its choices and adjust its behavior accordingly.</p> <p>By learning an accurate estimate of the state-value function, an agent can navigate its environment more effectively, prioritizing states that are likely to lead to higher cumulative rewards. This function is often approximated using deep neural networks in deep reinforcement learning, allowing the agent to generalize across complex and high-dimensional state spaces. Through iterative learning processes such as temporal difference methods or Monte Carlo estimation, the agent refines its understanding of the value of different states, ultimately improving its ability to make optimal decisions.</p>"},{"location":"basics/03_Action-Value%20Functions/","title":"Action-Value Functions","text":"<p>In deep reinforcement learning, the action-value function provides a way to evaluate the quality of taking a specific action in a given state while following a particular policy. Unlike the state-value function, which estimates the expected return from a state without considering specific actions, the action-value function, often denoted as \\(Q(s, a)\\), gives a more detailed assessment by accounting for the immediate action taken and its long-term consequences. This function helps an agent determine not just which states are valuable but also which actions within those states are most beneficial.</p> <p>Mathematically, the action-value function under a policy \\(\\pi\\) is defined as the expected cumulative reward obtained when the agent starts in state \\(s\\), takes action \\(a\\), and then follows the policy \\(\\pi\\) for all subsequent steps. This is expressed as:</p> \\[ Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid S_0 = s, A_0 = a \\right] \\] <p>where \\(R_t\\) represents the reward at time step \\(t\\), and \\(\\gamma\\) is the discount factor that determines how much future rewards contribute to the present estimate. The discount factor, typically between 0 and 1, controls the trade-off between immediate and long-term rewards, ensuring that future benefits do not dominate the decision-making process.</p> <p>The action-value function plays a crucial role in reinforcement learning algorithms, especially those based on value iteration and Q-learning. By learning accurate estimates of \\(Q(s, a)\\), an agent can make more informed decisions, selecting actions that maximize expected rewards over time. In practical applications, deep reinforcement learning methods approximate the action-value function using neural networks, particularly in deep Q-networks (DQN), where the network predicts \\(Q(s, a)\\) values for all possible actions given a state. Through iterative updates using techniques such as temporal difference learning, the agent refines its estimates, progressively improving its ability to select optimal actions. </p> <p>By leveraging the action-value function, reinforcement learning agents can efficiently navigate complex environments, learning to take actions that lead to the highest long-term rewards. This function serves as the foundation for many policy optimization techniques, as it provides a direct measure of action quality, helping the agent balance exploration and exploitation while learning an effective decision-making strategy.</p>"},{"location":"basics/03_Action-Value%20Functions/#relationship-with-state-value-functions","title":"Relationship with State-Value Functions","text":"<p>State-value functions and action-value functions are closely related, as they both aim to quantify the expected cumulative reward an agent can obtain in a reinforcement learning setting. While the state-value function, \\(V^\\pi(s)\\), evaluates the quality of a state under a given policy, the action-value function, \\(Q^\\pi(s, a)\\), assesses the value of taking a specific action in a state before following the policy thereafter. Their connection allows an agent to transition between evaluating states and selecting actions, facilitating decision-making in reinforcement learning.</p> <p>The relationship between these two functions is formalized through an expectation over the policy\u2019s action distribution. Given a state \\(s\\), the state-value function can be expressed in terms of the action-value function as follows:</p> \\[ V^\\pi(s) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s)} [Q^\\pi(s, a)] \\] <p>This equation states that the value of a state under policy \\(\\pi\\) is the expected value of the action-value function, where the expectation is taken over all possible actions the policy might select. If the policy is deterministic, meaning it always chooses the same action in a given state, the relationship simplifies to:</p> \\[ V^\\pi(s) = Q^\\pi(s, \\pi(s)) \\] <p>which means that the state-value function is simply the action-value function evaluated at the action dictated by the policy.</p> <p>Conversely, the action-value function can be related to the state-value function through the Bellman equation. When an agent takes action \\(a\\)in state\\(s\\), receives an immediate reward \\(R\\), and transitions to a new state \\(s'\\), the action-value function can be rewritten as:</p> \\[ Q^\\pi(s, a) = \\mathbb{E} [R + \\gamma V^\\pi(s') \\mid S_t = s, A_t = a] \\] <p>This equation expresses the action-value function in terms of the immediate reward and the discounted expected value of the next state. Since \\(V^\\pi(s')\\) itself depends on \\(Q^\\pi(s', a')\\), this recursive relationship forms the foundation for dynamic programming methods such as Q-learning and SARSA.</p> <p>By leveraging this relationship, reinforcement learning algorithms can switch between estimating state values and action values, using one to refine the other. In practice, many deep reinforcement learning approaches, such as deep Q-networks (DQN), focus on learning \\(Q(s, a)\\) directly, while policy gradient methods often rely on \\(V(s)\\) to estimate expected returns and improve policy updates. This interplay between the two functions allows for efficient learning and optimal decision-making in complex environments.</p>"},{"location":"code/Agent/","title":"SimpleAgent","text":"<p>The <code>SimpleAgent</code> class provides an agent that interacts with an environment and takes random actions without any learning. It supports discrete and continuous action spaces and can store experiences in memory.</p>"},{"location":"code/Agent/#class-diagram","title":"Class Diagram","text":"<pre><code>classDiagram\n    class SimpleAgent{\n        -AbstractEnvironment env\n        -int max_steps\n        -Memory memory\n        -int batch_size\n        -ndarray state\n        -float score\n        -int current_step\n        -dict agent_params\n        +__init__(env: AbstractEnvironment, eps: float, memory_size: int, batch_size: int, action_space_discrete: bool, max_steps: int, device: str)\n        +reset()\n        +update_state() tuple(ndarray, ndarray)\n        +update_memory(states: ndarray, actions: ndarray, rewards: ndarray, next_states: ndarray, dones: ndarray)\n        +choose_action() ndarray\n        +do_step(actions: ndarray) tuple(ndarray, ndarray, ndarray, ndarray)\n        +learn()\n        +play() float\n        +save()\n    }</code></pre>"},{"location":"code/Agent/#attributes","title":"Attributes","text":"Attribute Type Description env <code>AbstractEnvironment</code> The environment in which the agent operates. max_steps <code>int</code> The maximum number of steps the agent can take per episode. memory <code>Memory</code> Experience replay memory for storing state transitions (optional). batch_size <code>int</code> The batch size used when sampling from memory. state <code>ndarray</code> The current state of the agent. score <code>float</code> The total score accumulated during an episode. current_step <code>int</code> The current step within an episode. agent_params <code>dict</code> Additional parameters for the agent."},{"location":"code/Agent/#methods","title":"Methods","text":"Method Arguments Returns Description <code>reset</code> Reset the environment and agent state. <code>update_state</code> <code>(ndarray, ndarray)</code> Update the agent's state from the environment and return the previous and new state. <code>update_memory</code> states: ndarray, actions: ndarray, rewards: ndarray, next_states: ndarray, dones: ndarray Store experience in memory. <code>choose_action</code> <code>ndarray</code> Choose an action randomly. <code>do_step</code> actions: <code>ndarray</code> <code>(ndarray, ndarray, ndarray, ndarray)</code> Perform a step in the environment and return state transitions and rewards. <code>learn</code> Dummy learning function (not implemented in this agent). <code>play</code> <code>float</code> Run an episode by interacting with the environment and return the accumulated score. <code>save</code> Save the agent\u2019s status (not implemented)."},{"location":"code/Environment/","title":"Environments","text":"<p>The Environment classes are a wrapper for interacting with environment like Gymnasium or Unity ML Agent environments.</p>"},{"location":"code/Environment/#class-diagram","title":"Class Diagram","text":"<pre><code>classDiagram\n    class AbstractEnvironment{\n        -ndarray current_states\n        -any env\n        -dict env_params\n        -ndarray last_actions\n        -ndarray last_rewards\n        +__init__(name: str)\n        +close()\n        +get_action_shape() tuple[int]\n        +get_current_states() ndarray\n        +get_last_actions() ndarray\n        +get_last_rewards() ndarray\n        +get_num_agents() int\n        +get_state_shape() tuple[int]\n        +is_action_space_continuous() bool\n        +is_done() ndarray[int]\n        +is_state_space_continuous() bool\n        +reset() void\n        +step(actions: ndarray) ndarray[float]\n    }</code></pre>"},{"location":"code/Environment/#variables","title":"Variables","text":"<ul> <li>\\(\\mathcal{N}_a\\): Number of agents</li> <li>\\(\\mathcal{D}_A\\): action space dimension</li> <li>\\(\\mathcal{D}_S\\): state space dimension</li> </ul>"},{"location":"code/Environment/#attributes","title":"Attributes","text":"Attribute Type Description current_states ndarray of shape \\(N_a \\times \\mathcal{D}_S\\) The current states of the environment (last observations made by the agents) env Environment object The environment env_params dict Parameters that were used to initialize the environment last_actions ndarray of shape \\(N_a \\times \\mathcal{D}_A\\) Last actions taken by the agents last_rewards ndarray of shape \\(N_a \\times 1\\) Last rewards received by the agents"},{"location":"code/Environment/#methods","title":"Methods","text":"Method Arguments Returns Description close Close the environment get_action_shape Shape of action Get the shape \\(N_a \\times \\mathcal{D}_A\\) of actions that this environment accepts. get_current_state State represented by a \\(\\mathcal{N}_a \\times \\mathcal{D}_S\\) vector. Get the current state of the environment. get_last_actions Last actions taken by the agents represented by a \\(\\mathcal{N}_a \\times \\mathcal{D}_A\\) vector. Get the last actions taken by the agents. get_last_rewards Last rewards represented by a \\(\\mathcal{N}_a \\times 1\\) vector. Get the last rewards of the agents received by the last actions taken. get_num_actions Number of available actions Get the number of available actions in this environment. If the environment is continuous this method will return infinity. get_num_agents Number of agents Get the number of agents \\(\\mathcal{N}_a\\) in this environment get_state_shape Shape of state Get the shape \\(N_a \\times \\mathcal{D}_S\\) of the state of this environment is_action_space_continuous True if the action space is continuous, False if the action space is discrete. Is action space continuous? is_done Binary integer array containing zeros and ones of shape \\(\\mathcal{N}_a \\times 1\\) indicating for each agent if can still act or if it reached its goal Check if agents can still act of if they reached their goal is_state_space_continuous True if the state space is continuous, False if the state space is discrete. Is state space continuous? render RGB array of rendered environment Render the environment reset Reset the environment step actions: Action vector of shape \\(\\mathcal{N}_a \\times \\mathcal{D}_A\\) Rewards for each agent represented by a float array of shape \\(\\mathcal{N}_a \\times 1\\) Each agent executes an action and receives a reward."},{"location":"code/Trainer/","title":"Trainer","text":"<p>The <code>Trainer</code> class is responsible for training an agent in a given environment. It manages training episodes and stores performance metrics.</p>"},{"location":"code/Trainer/#class-diagram","title":"Class Diagram","text":"<pre><code>classDiagram\n    class Trainer{\n        -env\n        -agent\n        -list scores\n        -int avg_epochs_score\n        +__init__(env, agent, device: str)\n        +reset()\n        +train(n_epochs: int, eps_start: float, eps_end: float, eps_decay: float)\n        +plot_scores()\n    }</code></pre>"},{"location":"code/Trainer/#attributes","title":"Attributes","text":"Attribute Type Description env <code>object</code> The environment in which the agent is trained. agent <code>object</code> The agent being trained. scores <code>list</code> A list containing the scores from each episode. avg_epochs_score <code>int</code> The number of epochs used for calculating the moving average of scores."},{"location":"code/Trainer/#methods","title":"Methods","text":"Method Arguments Returns Description <code>reset</code> Reset training progress by clearing scores. <code>train</code> n_epochs: <code>int</code>, eps_start: <code>float</code>, eps_end: <code>float</code>, eps_decay: <code>float</code> Train the agent for a specified number of epochs using an epsilon-greedy strategy. <code>plot_scores</code> Plot the training scores over time."},{"location":"networks/DeepQNetwork/","title":"DeepQNetwork","text":"<p>The <code>DeepQNetwork</code> class implements a simple feed-forward neural network to map state information to actions. It is used in reinforcement learning to approximate Q-values.</p>"},{"location":"networks/DeepQNetwork/#class-diagram","title":"Class Diagram","text":"<pre><code>classDiagram\n    class DeepQNetwork{\n        -int n_states\n        -int n_actions\n        -torch.nn.Sequential net\n        +__init__(n_states: int, n_actions: int, hidden_dims: list, activation: nn.Module, norm: nn.Module, seed: int)\n        +forward(x: torch.Tensor) torch.Tensor\n        +get_best_choice(actions: torch.Tensor) tuple(torch.Tensor, torch.Tensor)\n    }</code></pre>"},{"location":"networks/DeepQNetwork/#attributes","title":"Attributes","text":"Attribute Type Description n_states <code>int</code> The number of state features. n_actions <code>int</code> The number of possible actions. net <code>torch.nn.Sequential</code> The neural network model mapping states to actions."},{"location":"networks/DeepQNetwork/#methods","title":"Methods","text":"Method Arguments Returns Description <code>forward</code> x: <code>torch.Tensor</code> <code>torch.Tensor</code> Pass the state vector through the network and obtain action scores. <code>get_best_choice</code> actions: <code>torch.Tensor</code> <code>(torch.Tensor, torch.Tensor)</code> Retrieve the best action index and corresponding value."}]}